{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1Z63fa24v98cA9RfoZsdI6gQHM6agS0lA","timestamp":1742443638450},{"file_id":"1jZLFqeQ4lWXzQdM-TE5ZNbIn7FV2l8nc","timestamp":1741224946423}],"gpuType":"A100","machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["!pip install -q transformers datasets accelerate bitsandbytes google-cloud-storage gguf langchain_community fastapi uvicorn pyngrok nest_asyncio"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zQXfJn6ZLzH8","executionInfo":{"status":"ok","timestamp":1744251642707,"user_tz":-360,"elapsed":84529,"user":{"displayName":"Salah Uddin","userId":"15536866033402565836"}},"outputId":"1863a933-5ff2-4106-86d6-b78863f003fe"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.2/76.2 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m70.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m423.3/423.3 kB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m51.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m46.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', RemoteDisconnected('Remote end closed connection without response'))': /packages/8a/6d/44ad094874c6f1b9c654f8ed939590bdc408349f137f9b98a3a23ccec411/nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl\u001b[0m\u001b[33m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n","\u001b[0m"]}]},{"cell_type":"code","source":["import os\n","from google.colab import drive\n","from google.cloud import storage\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","# 1. Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# 2. Set environment variable for credentials\n","credentials_path = \"/content/drive/MyDrive/gcp-key.json\"  # Path to your key file\n","os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"] = credentials_path\n","\n","# 3. Initialize GCS client\n","storage_client = storage.Client()\n","bucket_name = \"llm-test-bucket-2025\"  # Replace with your bucket name\n","bucket = storage_client.bucket(bucket_name)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hSQPNmzjMRwW","executionInfo":{"status":"ok","timestamp":1744251685792,"user_tz":-360,"elapsed":33963,"user":{"displayName":"Salah Uddin","userId":"15536866033402565836"}},"outputId":"bcdf1cde-c57a-4f53-a8ba-aa1eff17d198"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# 4. Save tokenizer and model to a local directory\n","model_dir = \"model_weights_4bit\"  # A local directory to save the model and tokenizer\n","os.makedirs(model_dir, exist_ok=True)"],"metadata":{"id":"49uEY1sUMffQ","executionInfo":{"status":"ok","timestamp":1744251705004,"user_tz":-360,"elapsed":4,"user":{"displayName":"Salah Uddin","userId":"15536866033402565836"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["import os\n","\n","# Define local directory where model files will be downloaded\n","def download_from_gcs(gcs_prefix, local_dir):\n","    \"\"\"Download all files from GCS folder to local directory.\"\"\"\n","    blobs = bucket.list_blobs(prefix=gcs_prefix)\n","    for blob in blobs:\n","        local_filepath = os.path.join(local_dir, os.path.basename(blob.name))\n","        print(f\"Downloading {blob.name} to {local_filepath}...\")\n","        blob.download_to_filename(local_filepath)\n","\n","# Download model weights from GCS\n","download_from_gcs(\"model_weights\", model_dir)\n","\n","print(\"Model files downloaded successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0gdO507QMinU","executionInfo":{"status":"ok","timestamp":1744251802019,"user_tz":-360,"elapsed":90260,"user":{"displayName":"Salah Uddin","userId":"15536866033402565836"}},"outputId":"513c21c9-8603-4e43-aadd-243444f7beba"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading model_weights/config.json to model_weights_4bit/config.json...\n","Downloading model_weights/generation_config.json to model_weights_4bit/generation_config.json...\n","Downloading model_weights/model-00001-of-00002.safetensors to model_weights_4bit/model-00001-of-00002.safetensors...\n","Downloading model_weights/model-00002-of-00002.safetensors to model_weights_4bit/model-00002-of-00002.safetensors...\n","Downloading model_weights/model.safetensors.index.json to model_weights_4bit/model.safetensors.index.json...\n","Downloading model_weights/special_tokens_map.json to model_weights_4bit/special_tokens_map.json...\n","Downloading model_weights/tokenizer.json to model_weights_4bit/tokenizer.json...\n","Downloading model_weights/tokenizer_config.json to model_weights_4bit/tokenizer_config.json...\n","Downloading model_weights_tinyllama/config.json to model_weights_4bit/config.json...\n","Downloading model_weights_tinyllama/generation_config.json to model_weights_4bit/generation_config.json...\n","Downloading model_weights_tinyllama/model.safetensors to model_weights_4bit/model.safetensors...\n","Downloading model_weights_tinyllama/special_tokens_map.json to model_weights_4bit/special_tokens_map.json...\n","Downloading model_weights_tinyllama/tokenizer.json to model_weights_4bit/tokenizer.json...\n","Downloading model_weights_tinyllama/tokenizer.model to model_weights_4bit/tokenizer.model...\n","Downloading model_weights_tinyllama/tokenizer_config.json to model_weights_4bit/tokenizer_config.json...\n","Model files downloaded successfully!\n"]}]},{"cell_type":"code","source":["from transformers import AutoModelForCausalLM, AutoTokenizer\n","import torch\n","\n","# Define paths\n","model_path = model_dir  # Path to downloaded model files\n","\n","# Load tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(model_path)\n","\n","# Load model (ensure it uses 8-bit quantization)\n","model = AutoModelForCausalLM.from_pretrained(\n","    model_path,\n","    torch_dtype=torch.float16,  # Use float16 for efficiency\n","    device_map=\"auto\"  # Automatically map to available GPU\n",")\n","\n","print(\"Model loaded successfully!\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8JNk9hTdNDXa","executionInfo":{"status":"ok","timestamp":1744251821501,"user_tz":-360,"elapsed":5263,"user":{"displayName":"Salah Uddin","userId":"15536866033402565836"}},"outputId":"76611f9a-2ae0-4ba4-de68-3fcf3806bf19"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Model loaded successfully!\n"]}]},{"cell_type":"code","source":["import uvicorn\n","from pyngrok import ngrok\n","import nest_asyncio\n","from fastapi import FastAPI, HTTPException\n","from pydantic import BaseModel\n","\n","from google.colab import userdata\n","NGROK_AUTH_TOKEN = userdata.get('NGROK_AUTH_TOKEN')\n","HUGGING_FACE_TOKEN = userdata.get('HF_TOKEN')\n","\n","# Allow nested event loops (required for Colab)\n","nest_asyncio.apply()\n","\n","app = FastAPI()\n","\n","# Define a request model for the prompt\n","class MessagesRequest(BaseModel):\n","    messages: list  # List of dictionaries with \"role\" and \"content\" keys\n","\n","@app.post(\"/generate\")\n","async def generate(request: MessagesRequest):\n","    try:\n","        # Format the messages using the tokenizer's chat template\n","        prompt = tokenizer.apply_chat_template(request.messages, tokenize=False, add_generation_prompt=True)\n","\n","        # Tokenize the prompt\n","        inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n","\n","        # Generate the response\n","        generated_ids = model.generate(\n","            inputs.input_ids,\n","            max_new_tokens=5000,  # Adjust as needed\n","            temperature=0.7,      # Lower values make the output more deterministic\n","            top_k=50,             # Lower k focuses on higher probability tokens\n","            top_p=0.95,           # Lower values make the output more focused\n","            do_sample=True        # Enable sampling\n","        )\n","\n","        generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, generated_ids)]\n","\n","        # response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n","        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","\n","        # Extract only the assistant's response (remove the prompt)\n","        # assistant_response = response.split(prompt)[-1].strip()\n","\n","        return {\"response\": response}\n","    except Exception as e:\n","        raise HTTPException(status_code=500, detail=str(e))\n","\n","# Set your ngrok authtoken\n","ngrok.set_auth_token(NGROK_AUTH_TOKEN)\n","\n","# Start ngrok tunnel\n","public_url = ngrok.connect(8000).public_url\n","print(f\"Public URL: {public_url}\")\n","\n","# Run the app in the background\n","uvicorn.run(app, host=\"0.0.0.0\", port=8000)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xhK6Uz6cNGhG","outputId":"5cce0caf-ea2c-4df8-b3bd-2c2831d60d76"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[]},{"output_type":"stream","name":"stderr","text":["INFO:     Started server process [4812]\n","INFO:     Waiting for application startup.\n","INFO:     Application startup complete.\n","INFO:     Uvicorn running on http://0.0.0.0:8000 (Press CTRL+C to quit)\n"]},{"output_type":"stream","name":"stdout","text":["Public URL: https://a4ed-34-132-149-194.ngrok-free.app\n"]},{"output_type":"stream","name":"stderr","text":["WARNING:pyngrok.process.ngrok:t=2025-04-10T02:24:06+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n","/usr/local/lib/python3.11/dist-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["INFO:     35.222.0.107:0 - \"POST /generate HTTP/1.1\" 200 OK\n","INFO:     35.222.0.107:0 - \"POST /generate HTTP/1.1\" 200 OK\n","INFO:     35.222.0.107:0 - \"POST /generate HTTP/1.1\" 200 OK\n"]}]}]}